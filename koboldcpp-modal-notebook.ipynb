{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Welcome to Modal Koboldcpp notebooks!\n",
        "\n",
        "## This is an unofficial notebook simply made to run local models in Modal. It's not perfect but it works\n",
        "\n",
        "## Recommended Compute Profile:\n",
        "- CPU: 2 cores\n",
        "- Ram: 2GB\n",
        "- GPU: based of the model size (e.g: Valkryie v2 with Q6K_L quants and 16K context you can use Nvidia L40S)\n",
        "- You can use less CPU and Ram like: 1 core 512MB if you really want to save credits\n",
        "\n",
        "## What model can i run?\n",
        "- Basically almost anything.\n",
        "\n",
        "## So i can run deepseek? \n",
        "- Technically yeah. But that would be expensive and i don't think Modal's free credits can last long enough for a session of roleplay with deepseek (most of your compute time wasted on downloading and compiling the model anyway)\n",
        "\n",
        "## So what can i *exactly* run?\n",
        "- With free credits, any model below 100B parameters should run comfortably for a moderate amount of time here.\n",
        "- You can use this [huggingface vram calc](https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator) (calc is calculator in short) to see how many vram a model with certain context size and a certain quantization would need. If you're very unsure, just use H100 or H200"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Install the necessary dependencies and installing koboldcpp\n",
        "import json\n",
        "!echo Downloading KoboldCpp, please wait...\n",
        "!wget -O dlfile.tmp https://kcpplinux.concedo.workers.dev && mv dlfile.tmp koboldcpp_linux\n",
        "!test -f koboldcpp_linux && echo Download Successful || echo Download Failed\n",
        "!chmod +x ./koboldcpp_linux\n",
        "!apt update\n",
        "!apt install aria2 -y\n",
        "\n",
        "# Change the model link to be any model link from huggingface (.gguf only)\n",
        "modellink = \"https://huggingface.co/bartowski/invisietch_L3.3-Ignition-v0.1-70B-GGUF/resolve/main/invisietch_L3.3-Ignition-v0.1-70B-Q6_K/invisietch_L3.3-Ignition-v0.1-70B-Q6_K-00001-of-00002.gguf?download=true\"\n",
        "Context = \"16384\" # Change the modet's maximum context size\n",
        "Layers = \"99\" # Change the number of layers to offload to GPU (0 for CPU only)\n",
        "modelName = \"TheDrummer/Valkyrie-49B-v2-Q6_K_L\" #Change this to any model name you want\n",
        "Instruct_Preset = \"llama-3\" # Change this to match your model's instruction format\n",
        "# Supported instructs: \n",
        "# alpaca,\n",
        "# vicuna, \n",
        "# llama-3,\n",
        "# chatml,\n",
        "# command-r,\n",
        "# mistral,\n",
        "# metharme,\n",
        "# gemma2\n",
        "# use \"custom\" if you wish to use a custome one\n",
        "\n",
        "# If your instruct preset is not present, you can make/paste a custom one\n",
        "# edit the custom instruct preset here\n",
        "custom_instruct = {\n",
        "    \"custom\": {\n",
        "        \"system_start\": \"\",\n",
        "        \"system_end\": \"\",\n",
        "        \"user_start\": \"\",\n",
        "        \"user_end\": \"\",\n",
        "        \"assistant_start\": \"\",\n",
        "        \"assistant_end\": \"\",\n",
        "    }\n",
        "}\n",
        "\n",
        "premade_instruct = {\n",
        "    \"alpaca\": {\n",
        "        \"system_start\": \"\\n### Input: \",\n",
        "        \"system_end\": \"\",\n",
        "        \"user_start\": \"\\n### Instruction: \",\n",
        "        \"user_end\": \"\",\n",
        "        \"assistant_start\": \"\\n### Response: \",\n",
        "        \"assistant_end\": \"\",\n",
        "    },\n",
        "    \"vicuna\": {\n",
        "        \"system_start\": \"\\nSYSTEM: \",\n",
        "        \"system_end\": \"\",\n",
        "        \"user_start\": \"\\nUSER: \",\n",
        "        \"user_end\": \"\",\n",
        "        \"assistant_start\": \"\\nASSISTANT: \",\n",
        "        \"assistant_end\": \"\",\n",
        "    },\n",
        "    \"llama-3\": {\n",
        "        \"system_start\": \"<|start_header_id|>system<|end_header_id|>\\n\\n\",\n",
        "        \"system_end\": \"<|eot_id|>\",\n",
        "        \"user_start\": \"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "        \"user_end\": \"<|eot_id|>\",\n",
        "        \"assistant_start\": \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        "        \"assistant_end\": \"<|eot_id|>\",\n",
        "    },\n",
        "    \"chatml\": {\n",
        "        \"system_start\": \"<|im_start|>system\",\n",
        "        \"system_end\": \"<|im_end|>\",\n",
        "        \"user_start\": \"<|im_start|>user\",\n",
        "        \"user_end\": \"<|im_end|>\",\n",
        "        \"assistant_start\": \"<|im_start|>assistant\",\n",
        "        \"assistant_end\": \"<|im_end|>\",\n",
        "    },\n",
        "    \"command-r\": {\n",
        "        \"system_start\": \"<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>\",\n",
        "        \"system_end\": \"<|END_OF_TURN_TOKEN|>\",\n",
        "        \"user_start\": \"<|START_OF_TURN_TOKEN|><|USER_TOKEN|>\",\n",
        "        \"user_end\": \"<|END_OF_TURN_TOKEN|>\",\n",
        "        \"assistant_start\": \"<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\",\n",
        "        \"assistant_end\": \"<|END_OF_TURN_TOKEN|>\",\n",
        "    },\n",
        "    \"mistral\":  {\n",
        "      \"system_start\": \"\",\n",
        "      \"system_end\": \"\",\n",
        "      \"user_start\": \"[INST] \",\n",
        "      \"user_end\": \"\",\n",
        "      \"assistant_start\": \" [/INST]\",\n",
        "      \"assistant_end\": \"</s> \"\n",
        "    },\n",
        "    \"gemma2\":{\n",
        "      \"system_start\": \"<start_of_turn>system\\n\",\n",
        "      \"system_end\": \"<end_of_turn>\\n\",\n",
        "      \"user_start\": \"<start_of_turn>user\\n\",\n",
        "      \"user_end\": \"<end_of_turn>\\n\",\n",
        "      \"assistant_start\": \"<start_of_turn>model\\n\",\n",
        "      \"assistant_end\": \"<end_of_turn>\\n\"\n",
        "    },\n",
        "    \"metharme\": {\n",
        "      \"system_start\": \"<|system|>\",\n",
        "      \"system_end\": \"\",\n",
        "      \"user_start\": \"<|user|>\",\n",
        "      \"user_end\": \"\",\n",
        "      \"assistant_start\": \"<|model>\",\n",
        "      \"assistant_end\": \"\"\n",
        "    }\n",
        "}\n",
        "\n",
        "#create instruct file\n",
        "with open(\"instruct.json\", \"w\") as f:\n",
        "    f.write(json.dumps(premade_instruct[Instruct_Preset], separators=(\",\", \":\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Download the model (or the first model part)\n",
        "modellink = \"https://huggingface.co/bartowski/invisietch_L3.3-Ignition-v0.1-70B-GGUF/resolve/main/invisietch_L3.3-Ignition-v0.1-70B-Q6_K/invisietch_L3.3-Ignition-v0.1-70B-Q6_K-00002-of-00002.gguf?download=true\"\n",
        "!aria2c -x 16 -s 16 -k 1M -o model-00001-of-00002.gguf --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $modellink\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Download the second part of the model (if any)\n",
        "modellink_2 = \"https://huggingface.co/bartowski/invisietch_L3.3-Ignition-v0.1-70B-GGUF/resolve/main/invisietch_L3.3-Ignition-v0.1-70B-Q6_K/invisietch_L3.3-Ignition-v0.1-70B-Q6_K-00002-of-00002.gguf?download=true\"\n",
        "!aria2c -x 16 -s 16 -k 1M -o model-00002-of-00002.gguf --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $modellink_2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Run without instruct preset\n",
        "!./koboldcpp_linux model.gguf --usecublas 0 mmq --multiuser --gpulayers $Layers --contextsize $Context --flashattention --hordemodelname $modelName --quiet --remotetunnel\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***\r\n",
            "Welcome to KoboldCpp - Version 1.98.1\r\n",
            "Downloading https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\r\n",
            "Attempting to start tunnel thread...\r\n",
            "Loading Chat Completions Adapter: /tmp/_MEITanz3n/kcpp_adapters/AutoGuess.json\r\n",
            "Chat Completions Adapter Loaded\r\n",
            "System: Linux #1 SMP Sun Jan 10 15:06:54 PST 2016 x86_64 \r\n",
            "Detected Available GPU Memory: 49140 MB\r\n",
            "Unable to determine available RAM\r\n",
            "Initializing dynamic library: koboldcpp_cublas.so\r\n",
            "==========\r\n",
            "Starting Cloudflare Tunnel for Linux, please wait...\r\n",
            "Namespace(admin=False, admindir='', adminpassword=None, analyze='', benchmark=None, blasbatchsize=512, blasthreads=27, chatcompletionsadapter='AutoGuess', cli=False, config=None, contextsize=16384, debugmode=-1, defaultgenamt=640, draftamount=8, draftgpulayers=999, draftgpusplit=None, draftmodel='', embeddingsgpu=False, embeddingsmaxctx=0, embeddingsmodel='', enableguidance=False, exportconfig='', exporttemplate='', failsafe=False, flashattention=True, forceversion=0, foreground=False, gpulayers=99, highpriority=False, hordeconfig=None, hordegenlen=0, hordekey='', hordemaxctx=0, hordemodelname='TheDrummer/Valkyrie-49B-v2-Q6_K_L', hordeworkername='', host='', ignoremissing=False, launch=False, lora=None, loramult=1.0, maingpu=-1, maxrequestsize=32, mmproj='', mmprojcpu=False, model=[], model_param='model.gguf', moecpu=0, moeexperts=-1, multiplayer=False, multiuser=1, noavx2=False, noblas=False, nobostoken=False, nocertify=False, nofastforward=False, nommap=False, nomodel=False, noshift=False, onready='', overridekv='', overridenativecontext=0, overridetensors='', password=None, port=5001, port_param=5001, preloadstory='', prompt='', promptlimit=100, quantkv=0, quiet=True, remotetunnel=True, ropeconfig=[0.0, 10000.0], savedatafile='', sdclamped=0, sdclampedsoft=0, sdclipg='', sdclipl='', sdconfig=None, sdconvdirect='off', sdflashattention=False, sdlora='', sdloramult=1.0, sdmodel='', sdnotile=False, sdphotomaker='', sdquant=0, sdt5xxl='', sdthreads=0, sdtiledvae=768, sdvae='', sdvaeauto=False, showgui=False, singleinstance=False, skiplauncher=False, smartcontext=False, ssl=None, tensor_split=None, threads=27, ttsgpu=False, ttsmaxlen=4096, ttsmodel='', ttsthreads=0, ttswavtokenizer='', unpack='', useclblast=None, usecpu=False, usecuda=['0', 'mmq'], usemlock=False, usemmap=False, useswa=False, usevulkan=None, version=False, visionmaxres=1024, websearch=False, whispermodel='')\r\n",
            "==========\r\n",
            "Loading Text Model: /root/model.gguf\r\n",
            "\r\n",
            "The reported GGUF Arch is: deci\r\n",
            "Arch Category: 0\r\n",
            "\r\n",
            "---\r\n",
            "Identified as GGUF model.\r\n",
            "Attempting to Load...\r\n",
            "---\r\n",
            "Using automatic RoPE scaling for GGUF. If the model has custom RoPE settings, they'll be used directly instead!\r\n",
            "System Info: AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \r\n",
            "CUDA MMQ: True\r\n",
            "---\r\n",
            "Initializing CUDA/HIP, please wait, the following step may take a few minutes (only for first launch)...\r\n",
            "---\r\n",
            "ggml_cuda_init: found 1 CUDA devices:\r\n",
            "  Device 0: NVIDIA L40S, compute capability 8.9, VMM: yes\r\n",
            "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA L40S) - 48078 MiB free\r\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 569 tensors from /root/model.gguf (version GGUF V3 (latest))\r\n",
            "print_info: file format = GGUF V3 (latest)\r\n",
            "print_info: file size   = 38.57 GiB (6.64 BPW) \r\n",
            "init_tokenizer: initializing tokenizer for type 2\r\n",
            "load: printing all EOG tokens:\r\n",
            "load:   - 128001 ('<|end_of_text|>')\r\n",
            "load:   - 128008 ('<|eom_id|>')\r\n",
            "load:   - 128009 ('<|eot_id|>')\r\n",
            "load: special tokens cache size = 256\r\n",
            "load: token to piece cache size = 0.7999 MB\r\n",
            "print_info: arch             = deci\r\n",
            "print_info: vocab_only       = 0\r\n",
            "print_info: n_ctx_train      = 131072\r\n",
            "print_info: n_embd           = 8192\r\n",
            "print_info: n_layer          = 80\r\n",
            "print_info: n_head           = [64, 64, 64, 64, 64, 64, 0, 0, 64, 64, 64, 0, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64, 64, 64, 64, 64, 64, 64, 64, 64]\r\n",
            "print_info: n_head_kv        = [8, 8, 8, 8, 8, 8, 0, 0, 8, 8, 8, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8]\r\n",
            "print_info: n_rot            = 128\r\n",
            "print_info: n_swa            = 0\r\n",
            "print_info: is_swa_any       = 0\r\n",
            "print_info: n_embd_head_k    = 128\r\n",
            "print_info: n_embd_head_v    = 128\r\n",
            "print_info: n_gqa            = [8, 8, 8, 8, 8, 8, 0, 0, 8, 8, 8, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8]\r\n",
            "print_info: n_embd_k_gqa     = [1024, 1024, 1024, 1024, 1024, 1024, 0, 0, 1024, 1024, 1024, 0, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1024, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024]\r\n",
            "print_info: n_embd_v_gqa     = [1024, 1024, 1024, 1024, 1024, 1024, 0, 0, 1024, 1024, 1024, 0, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1024, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024]\r\n",
            "print_info: f_norm_eps       = 0.0e+00\r\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\r\n",
            "print_info: f_clamp_kqv      = 0.0e+00\r\n",
            "print_info: f_max_alibi_bias = 0.0e+00\r\n",
            "print_info: f_logit_scale    = 0.0e+00\r\n",
            "print_info: f_attn_scale     = 0.0e+00\r\n",
            "print_info: n_ff             = [14336, 28672, 28672, 28672, 28672, 28672, 14336, 14336, 28672, 28672, 28672, 17920, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 7168, 14336, 14336, 7168, 28672, 7168, 14336, 7168, 7168, 7168, 28672, 7168, 5632, 5632, 7168, 5632, 5632, 5632, 7168, 7168, 2816, 2816, 5632, 5632, 2816, 2816, 5632, 2816, 2816, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672]\r\n",
            "print_info: n_expert         = 0\r\n",
            "print_info: n_expert_used    = 0\r\n",
            "print_info: causal attn      = 1\r\n",
            "print_info: pooling type     = 0\r\n",
            "print_info: rope type        = 0\r\n",
            "print_info: rope scaling     = linear\r\n",
            "print_info: freq_base_train  = 500000.0\r\n",
            "print_info: freq_scale_train = 1\r\n",
            "print_info: n_ctx_orig_yarn  = 131072\r\n",
            "print_info: rope_finetuned   = unknown\r\n",
            "print_info: model type       = 70B\r\n",
            "print_info: model params     = 49.87 B\r\n",
            "print_info: general.name     = Valkyrie 49B v2\r\n",
            "print_info: vocab type       = BPE\r\n",
            "print_info: n_vocab          = 128256\r\n",
            "print_info: n_merges         = 280147\r\n",
            "print_info: BOS token        = 128000 '<|begin_of_text|>'\r\n",
            "print_info: EOS token        = 128009 '<|eot_id|>'\r\n",
            "print_info: EOT token        = 128009 '<|eot_id|>'\r\n",
            "print_info: EOM token        = 128008 '<|eom_id|>'\r\n",
            "print_info: PAD token        = 128004 '<|finetune_right_pad_id|>'\r\n",
            "print_info: LF token         = 198 '\u010a'\r\n",
            "print_info: EOG token        = 128001 '<|end_of_text|>'\r\n",
            "print_info: EOG token        = 128008 '<|eom_id|>'\r\n",
            "print_info: EOG token        = 128009 '<|eot_id|>'\r\n",
            "print_info: max token length = 256\r\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = false)\r\n",
            "load_tensors: relocated tensors: 0 of 648\r\n",
            "load_tensors: offloading 80 repeating layers to GPU\r\n",
            "load_tensors: offloading output layer to GPU\r\n",
            "load_tensors: offloaded 81/81 layers to GPU\r\n",
            "load_tensors:    CUDA_Host model buffer size =  1064.62 MiB\r\n",
            "load_tensors:        CUDA0 model buffer size = 38435.56 MiB\r\n",
            "load_all_data: using async uploads for device CUDA0, buffer type CUDA0, backend CUDA0\r\n",
            ".................................................................................................\r\n",
            "Automatic RoPE Scaling: Using (scale:1.000, base:500000.0).\r\n",
            "llama_context: constructing llama_context\r\n",
            "llama_context: n_seq_max     = 1\r\n",
            "llama_context: n_ctx         = 16512\r\n",
            "llama_context: n_ctx_per_seq = 16512\r\n",
            "llama_context: n_batch       = 512\r\n",
            "llama_context: n_ubatch      = 512\r\n",
            "llama_context: causal_attn   = 1\r\n",
            "llama_context: flash_attn    = 1\r\n",
            "llama_context: kv_unified    = true\r\n",
            "llama_context: freq_base     = 500000.0\r\n",
            "llama_context: freq_scale    = 1\r\n",
            "llama_context: n_ctx_per_seq (16512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\r\n",
            "set_abort_callback: call\r\n",
            "llama_context:  CUDA_Host  output buffer size =     0.49 MiB\r\n",
            "create_memory: n_ctx = 16640 (padded)\r\n",
            "llama_kv_cache:      CUDA0 KV buffer size =  3185.00 MiB\r\n",
            "llama_kv_cache: size = 3185.00 MiB ( 16640 cells,  80 layers,  1/1 seqs), K (f16): 1592.50 MiB, V (f16): 1592.50 MiB\r\n",
            "llama_context: enumerating backends\r\n",
            "llama_context: backend_ptrs.size() = 2\r\n",
            "llama_context: max_nodes = 4552\r\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\r\n",
            "llama_context: reserving full memory module\r\n",
            "llama_context:      CUDA0 compute buffer size =   282.50 MiB\r\n",
            "llama_context:  CUDA_Host compute buffer size =    48.51 MiB\r\n",
            "llama_context: graph nodes  = 1743\r\n",
            "llama_context: graph splits = 2\r\n",
            "Threadpool set to 27 threads and 27 blasthreads...\r\n",
            "attach_threadpool: call\r\n",
            "Starting model warm up, please wait a moment...\r\n",
            "Load Text Model OK: True\r\n",
            "Chat template heuristics failed to identify chat completions format. Alpaca will be used.\r\n",
            "Embedded KoboldAI Lite loaded.\r\n",
            "Embedded API docs loaded.\r\n",
            "======\r\n",
            "Active Modules: TextGeneration\r\n",
            "Inactive Modules: ImageGeneration VoiceRecognition MultimodalVision MultimodalAudio NetworkMultiplayer ApiKeyPassword WebSearchProxy TextToSpeech VectorEmbeddings AdminControl\r\n",
            "Enabled APIs: KoboldCppApi OpenAiApi OllamaApi\r\n",
            "Your remote Kobold API can be found at https://mit-materials-divx-studied.trycloudflare.com/api\r\n",
            "Your remote OpenAI Compatible API can be found at https://mit-materials-divx-studied.trycloudflare.com/v1\r\n",
            "======\r\n",
            "Your remote tunnel is ready, please connect to https://mit-materials-divx-studied.trycloudflare.com\r\n",
            "Token streaming was interrupted or aborted!\r\n",
            "[Errno 32] Broken pipe\r\n",
            "\r\n",
            "[01:31:56] CtxLimit:3014/16384, Amt:335/1000, Init:0.00s, Process:1.90s (1409.26T/s), Generate:18.37s (18.24T/s), Total:20.27s\r\n",
            "[01:32:45] CtxLimit:3182/16384, Amt:503/1000, Init:0.00s, Process:0.02s (52.63T/s), Generate:27.51s (18.28T/s), Total:27.53s\r\n",
            "[01:33:16] CtxLimit:3149/16384, Amt:470/1000, Init:0.00s, Process:0.02s (52.63T/s), Generate:25.71s (18.28T/s), Total:25.73s\r\n",
            "[01:33:46] CtxLimit:3082/16384, Amt:403/1000, Init:0.00s, Process:0.02s (52.63T/s), Generate:22.04s (18.29T/s), Total:22.05s\r\n",
            "[01:34:34] CtxLimit:3214/16384, Amt:535/1000, Init:0.00s, Process:0.02s (55.56T/s), Generate:29.18s (18.33T/s), Total:29.20s"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Run with instruct preset\n",
        "!./koboldcpp_linux model.gguf --usecublas 0 mmq --multiuser --gpulayers $Layers --contextsize $Context --flashattention --hordemodelname $modelName --quiet --remotetunnel --chatcompletionsadapter instruct.json\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}