{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Welcome to Modal Koboldcpp notebooks!\n",
        "\n",
        "## This is an unofficial notebook simply made to run local models in Modal. It's not perfect but it works\n",
        "\n",
        "## Recommended Compute Profile:\n",
        "- CPU: 2 cores\n",
        "- Ram: 2GB\n",
        "- GPU: based of the model size (e.g: Valkryie v2 with Q6K_L quants and 16K context you can use Nvidia L40S)\n",
        "- You can use less CPU and Ram like: 1 core 512MB if you really want to save credits\n",
        "\n",
        "## What model can i run?\n",
        "- Basically almost anything.\n",
        "\n",
        "## So i can run deepseek? \n",
        "- Technically yeah. But that would be expensive and i don't think Modal's free credits can last long enough for a session of roleplay with deepseek (most of your compute time wasted on downloading and compiling the model anyway)\n",
        "\n",
        "## So what can i *exactly* run?\n",
        "- With free credits, any model below 100B parameters should run comfortably for a moderate amount of time here.\n",
        "- You can use this [huggingface vram calc](https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator) (calc is calculator in short) to see how many vram a model with certain context size and a certain quantization would need. If you're very unsure, just use H100 or H200"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Install the necessary dependencies and installing koboldcpp\n",
        "import json\n",
        "!echo Downloading KoboldCpp, please wait...\n",
        "!wget -O dlfile.tmp https://kcpplinux.concedo.workers.dev && mv dlfile.tmp koboldcpp_linux\n",
        "!test -f koboldcpp_linux && echo Download Successful || echo Download Failed\n",
        "!chmod +x ./koboldcpp_linux\n",
        "!apt update\n",
        "!apt install aria2 -y\n",
        "\n",
        "# Change the model link to be any model link from huggingface (.gguf only)\n",
        "modellink = \"https://huggingface.co/bartowski/invisietch_L3.3-Ignition-v0.1-70B-GGUF/resolve/main/invisietch_L3.3-Ignition-v0.1-70B-Q6_K/invisietch_L3.3-Ignition-v0.1-70B-Q6_K-00001-of-00002.gguf?download=true\"\n",
        "Context = \"16384\" # Change the modet's maximum context size\n",
        "Layers = \"99\" # Change the number of layers to offload to GPU (0 for CPU only)\n",
        "Instruct_Preset = \"llama-3\" # Change this to match your model's instruction format\n",
        "# Supported instructs: \n",
        "# alpaca,\n",
        "# vicuna, \n",
        "# llama-3,\n",
        "# chatml,\n",
        "# command-r,\n",
        "# mistral,\n",
        "# metharme,\n",
        "# gemma2\n",
        "# use \"custom\" if you wish to use a custome one\n",
        "\n",
        "# If your instruct preset is not present, you can make/paste a custom one\n",
        "# edit the custom instruct preset here\n",
        "custom_instruct = {\n",
        "    \"custom\": {\n",
        "        \"system_start\": \"\",\n",
        "        \"system_end\": \"\",\n",
        "        \"user_start\": \"\",\n",
        "        \"user_end\": \"\",\n",
        "        \"assistant_start\": \"\",\n",
        "        \"assistant_end\": \"\",\n",
        "    }\n",
        "}\n",
        "\n",
        "premade_instruct = {\n",
        "    \"alpaca\": {\n",
        "        \"system_start\": \"\\n### Input: \",\n",
        "        \"system_end\": \"\",\n",
        "        \"user_start\": \"\\n### Instruction: \",\n",
        "        \"user_end\": \"\",\n",
        "        \"assistant_start\": \"\\n### Response: \",\n",
        "        \"assistant_end\": \"\",\n",
        "    },\n",
        "    \"vicuna\": {\n",
        "        \"system_start\": \"\\nSYSTEM: \",\n",
        "        \"system_end\": \"\",\n",
        "        \"user_start\": \"\\nUSER: \",\n",
        "        \"user_end\": \"\",\n",
        "        \"assistant_start\": \"\\nASSISTANT: \",\n",
        "        \"assistant_end\": \"\",\n",
        "    },\n",
        "    \"llama-3\": {\n",
        "        \"system_start\": \"<|start_header_id|>system<|end_header_id|>\\n\\n\",\n",
        "        \"system_end\": \"<|eot_id|>\",\n",
        "        \"user_start\": \"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "        \"user_end\": \"<|eot_id|>\",\n",
        "        \"assistant_start\": \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        "        \"assistant_end\": \"<|eot_id|>\",\n",
        "    },\n",
        "    \"chatml\": {\n",
        "        \"system_start\": \"<|im_start|>system\",\n",
        "        \"system_end\": \"<|im_end|>\",\n",
        "        \"user_start\": \"<|im_start|>user\",\n",
        "        \"user_end\": \"<|im_end|>\",\n",
        "        \"assistant_start\": \"<|im_start|>assistant\",\n",
        "        \"assistant_end\": \"<|im_end|>\",\n",
        "    },\n",
        "    \"command-r\": {\n",
        "        \"system_start\": \"<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>\",\n",
        "        \"system_end\": \"<|END_OF_TURN_TOKEN|>\",\n",
        "        \"user_start\": \"<|START_OF_TURN_TOKEN|><|USER_TOKEN|>\",\n",
        "        \"user_end\": \"<|END_OF_TURN_TOKEN|>\",\n",
        "        \"assistant_start\": \"<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\",\n",
        "        \"assistant_end\": \"<|END_OF_TURN_TOKEN|>\",\n",
        "    },\n",
        "    \"mistral\":  {\n",
        "      \"system_start\": \"\",\n",
        "      \"system_end\": \"\",\n",
        "      \"user_start\": \"[INST] \",\n",
        "      \"user_end\": \"\",\n",
        "      \"assistant_start\": \" [/INST]\",\n",
        "      \"assistant_end\": \"</s> \"\n",
        "    },\n",
        "    \"gemma2\":{\n",
        "      \"system_start\": \"<start_of_turn>system\\n\",\n",
        "      \"system_end\": \"<end_of_turn>\\n\",\n",
        "      \"user_start\": \"<start_of_turn>user\\n\",\n",
        "      \"user_end\": \"<end_of_turn>\\n\",\n",
        "      \"assistant_start\": \"<start_of_turn>model\\n\",\n",
        "      \"assistant_end\": \"<end_of_turn>\\n\"\n",
        "    },\n",
        "    \"metharme\": {\n",
        "      \"system_start\": \"<|system|>\",\n",
        "      \"system_end\": \"\",\n",
        "      \"user_start\": \"<|user|>\",\n",
        "      \"user_end\": \"\",\n",
        "      \"assistant_start\": \"<|model>\",\n",
        "      \"assistant_end\": \"\"\n",
        "    }\n",
        "}\n",
        "\n",
        "#create instruct file\n",
        "with open(\"instruct.json\", \"w\") as f:\n",
        "    f.write(json.dumps(premade_instruct[Instruct_Preset], separators=(\",\", \":\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Download the model (or the first model part)\n",
        "modellink = \"https://huggingface.co/bartowski/Sao10K_Llama-3.3-70B-Vulpecula-r1-GGUF/resolve/main/Sao10K_Llama-3.3-70B-Vulpecula-r1-Q6_K/Sao10K_Llama-3.3-70B-Vulpecula-r1-Q6_K-00001-of-00002.gguf?download=true\"\n",
        "modelName = modellink.split('/')[-1].split('.')[0]\n",
        "!aria2c -x 16 -s 16 -k 1M -o model-00001-of-00002.gguf --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $modellink\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Download the second part of the model (if any)\n",
        "modellink_2 = \"https://huggingface.co/bartowski/Sao10K_Llama-3.3-70B-Vulpecula-r1-GGUF/resolve/main/Sao10K_Llama-3.3-70B-Vulpecula-r1-Q6_K/Sao10K_Llama-3.3-70B-Vulpecula-r1-Q6_K-00002-of-00002.gguf?download=true\"\n",
        "!aria2c -x 16 -s 16 -k 1M -o model-00002-of-00002.gguf --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $modellink_2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Run without instruct preset\n",
        "!./koboldcpp_linux model-00001-of-00002.gguf --usecublas 0 mmq --multiuser --gpulayers $Layers --contextsize $Context --flashattention --hordemodelname $modelName --quiet --remotetunnel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Run with instruct preset\n",
        "!./koboldcpp_linux model-00001-of-00002.gguf --usecublas 0 mmq --multiuser --gpulayers $Layers --contextsize $Context --flashattention --hordemodelname $modelName --quiet --remotetunnel --chatcompletionsadapter instruct.json"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}